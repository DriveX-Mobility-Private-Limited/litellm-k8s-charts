# Default values for litellm.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  # Use "ghcr.io/berriai/litellm-database" for optimized image with database
  repository: ghcr.io/berriai/litellm-database
  pullPolicy: Always
  # Overrides the image tag whose default is the chart appVersion.
  # tag: "main-latest"
  tag: ""

imagePullSecrets: []
nameOverride: "litellm"
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}
podLabels: {}

# At the time of writing, the litellm docker image requires write access to the
#  filesystem on startup so that prisma can install some dependencies.
podSecurityContext: {}
securityContext: {}
  # capabilities:
  #   drop:
  #     - ALL
  # readOnlyRootFilesystem: false
  # runAsNonRoot: true
  # runAsUser: 1000

# A list of Kubernetes Secret objects that will be exported to the LiteLLM proxy
#  pod as environment variables.  These secrets can then be referenced in the
#  configuration file (or "litellm" ConfigMap) with `os.environ/<Env Var Name>`
environmentSecrets:
  - litellm-postgresql-credentials-prod

# A list of Kubernetes ConfigMap objects that will be exported to the LiteLLM proxy
#  pod as environment variables.  The ConfigMap kv-pairs can then be referenced in the
#  configuration file (or "litellm" ConfigMap) with `os.environ/<Env Var Name>`
environmentConfigMaps: []
  # - litellm-env-configmap

service:
  type: ClusterIP
  port: 4000

ingress:
  enabled: true
  className: "nginx"
  annotations:
    kubernetes.io/ingress.class: nginx
    kubernetes.io/tls-acme: "true"
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/use-forwarded-headers: "true"
    nginx.ingress.kubernetes.io/enable-underscores-in-headers: "true"
    nginx.ingress.kubernetes.io/proxy-pass-headers: "Authorization"
    nginx.ingress.kubernetes.io/proxy-set-headers: "/etc/nginx/proxy-headers"
    nginx.ingress.kubernetes.io/proxy-http-version: "1.1"
    nginx.ingress.kubernetes.io/proxy-buffering: "off"
    nginx.ingress.kubernetes.io/client-max-body-size: "0"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "36000s"
    nginx.ingress.kubernetes.io/proxy-redirect-off: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: "200m"    
  hosts:
    - host: llmproxy.drivex.in
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls:
   - secretName: litellm-prod-tls
     hosts:
       - llmproxy.drivex.in

# if set, use this secret for the master key; otherwise, autogenerate a new one
masterkeySecretName: "litellm-postgresql-credentials-prod"

# if set, use this secret key for the master key; otherwise, use the default key
masterkeySecretKey: "LITELLM_MASTER_KEY"

# The elements within proxy_config are rendered as config.yaml for the proxy
#  Examples: https://github.com/BerriAI/litellm/tree/main/litellm/proxy/example_config_yaml
#  Reference: https://docs.litellm.ai/docs/proxy/configs
proxy_config:
  general_settings:
    analytics: true
    master_key: os.environ/PROXY_MASTER_KEY
    admin_ui:
      username: os.environ/UI_USERNAME
      password: os.environ/UI_PASSWORD
    # Configure fallbacks
    # default_model_health: os.environ/DEFAULT_MODEL_HEALTH
    # fallbacks: [
    #   {
    #     # Gemini fallbacks
    #     "gemini-1.5-pro": ["gemini-1.5-flash", "gemini-pro", "gpt-4o"],
    #     "gemini-1.5-flash": ["gemini-pro", "gemini-1.5-pro", "gpt-4o-mini"]
    #     # "gemini-pro": ["gemini-1.5-flash", "gpt-3.5-turbo"],
    #     # "gemini-1.5-ultra": ["gemini-1.5-pro", "gemini-1.5-flash", "gpt-4o"],

    #     # # OpenAI fallbacks
    #     # "gpt-4o": ["gpt-4-turbo", "gemini-1.5-ultra", "gemini-1.5-pro"],
    #     # "gpt-4o-mini": ["gpt-3.5-turbo", "gemini-1.5-flash", "gemini-pro"],
    #     # "gpt-4-turbo": ["gpt-4o", "gemini-1.5-ultra"],
    #     # "gpt-3.5-turbo": ["gpt-4o-mini", "gemini-pro"]
    #   }
    # ]
    # # Configure health check
    # health_check_interval: 300
  model_list:
    # At least one model must exist for the proxy to start.
    - model_name: gpt-3.5-turbo
      litellm_params:
        model: openai/gpt-3.5-turbo
        api_key: os.environ/OPENAI_API_KEY
    - model_name: gpt-4.5
      litellm_params:
        model: openai/gpt-4.5
        api_key: os.environ/OPENAI_API_KEY
    - model_name: gpt-4-turbo
      litellm_params:
        model: openai/gpt-4-turbo
        api_key: os.environ/OPENAI_API_KEY
    - model_name: gpt-4
      litellm_params:
        model: openai/gpt-4
        api_key: os.environ/OPENAI_API_KEY
    - model_name: gpt-4-0613
      litellm_params:
        model: openai/gpt-4-0613
        api_key: os.environ/OPENAI_API_KEY
    - model_name: gpt-4-32k
      litellm_params:
        model: openai/gpt-4-32k
        api_key: os.environ/OPENAI_API_KEY
    - model_name: gpt-4-32k-0613
      litellm_params:
        model: openai/gpt-4-32k-0613
        api_key: os.environ/OPENAI_API_KEY
    - model_name: gpt-3.5
      litellm_params:
        model: openai/gpt-3.5
        api_key: os.environ/OPENAI_API_KEY
    - model_name: gpt-3
      litellm_params:
        model: openai/gpt-3
        api_key: os.environ/OPENAI_API_KEY
    - model_name: o3-mini
      litellm_params:
        model: openai/o3-mini
        api_key: os.environ/OPENAI_API_KEY
    - model_name: o1
      litellm_params:
        model: openai/o1
        api_key: os.environ/OPENAI_API_KEY
    - model_name: o1-mini
      litellm_params:
        model: openai/o1-mini
        api_key: os.environ/OPENAI_API_KEY
    - model_name: o1-pro
      litellm_params:
        model: openai/o1-pro
        api_key: os.environ/OPENAI_API_KEY
    - model_name: gpt-4o
      litellm_params:
        model: openai/gpt-4o
        api_key: os.environ/OPENAI_API_KEY
    - model_name: gpt-4o-mini
      litellm_params:
        model: openai/gpt-4o-mini
        api_key: os.environ/OPENAI_API_KEY
    - model_name: gpt-4o-mini-audio
      litellm_params:
        model: openai/gpt-4o-mini-audio
        api_key: os.environ/OPENAI_API_KEY
    - model_name: whisper-1
      litellm_params:
        model: whisper-1
        api_key: os.environ/OPENAI_API_KEY  
  # Direct Gemini models
    - model_name: gemini-pro
      litellm_params:
        model: gemini/gemini-pro
        api_key: os.environ/GOOGLE_API_KEY
    - model_name: gemini-pro-vision
      litellm_params:
        model: gemini/gemini-pro-vision
        api_key: os.environ/GOOGLE_API_KEY
    - model_name: gemini-1.5-pro
      litellm_params:
        model: gemini/gemini-1.5-pro
        api_key: os.environ/GOOGLE_API_KEY
    - model_name: gemini-1.5-flash
      litellm_params:
        model: gemini/gemini-1.5-flash
        api_key: os.environ/GOOGLE_API_KEY
    - model_name: gemini-1.5-pro-vision
      litellm_params:
        model: gemini/gemini-1.5-pro-vision
        api_key: os.environ/GOOGLE_API_KEY
    - model_name: gemini-1.5-flash-preview
      litellm_params:
        model: gemini/gemini-1.5-flash-preview
        api_key: os.environ/GOOGLE_API_KEY
    - model_name: gemini-2.0-flash
      litellm_params:
        model: gemini/gemini-2.0-flash
        api_key: os.environ/GEMINI_API_KEY
    - model_name: gemini-2.5-pro-experimental
      litellm_params:
        model: gemini/gemini-2.5-pro-experimental
        api_key: os.environ/GEMINI_API_KEY

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

# Additional volumes on the output Deployment definition.
volumes: []
# - name: foo
#   secret:
#     secretName: mysecret
#     optional: false

# Additional volumeMounts on the output Deployment definition.
volumeMounts: []
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true

nodeSelector:
  if-llm-workload: "true"

tolerations:
  - key: "if-monitoring-pod"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
  - key: "if-llm-workload"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
    
affinity: {}

db:
  # Use an existing postgres server/cluster
  useExisting: false

 # How to connect to the existing postgres server/cluster
  endpoint: localhost
  database: litellm
  url: postgresql://$(DATABASE_USERNAME):$(DATABASE_PASSWORD)@$(DATABASE_HOST)/$(DATABASE_NAME)
  secret:
    name: postgres
    usernameKey: username
    passwordKey: password

  # Use the Stackgres Helm chart to deploy an instance of a Stackgres cluster.
  #  The Stackgres Operator must already be installed within the target
  #  Kubernetes cluster.
  # TODO: Stackgres deployment currently unsupported
  useStackgresOperator: false

  # Use the Postgres Helm chart to create a single node, stand alone postgres
  #  instance.  See the "postgresql" top level key for additional configuration.
  deployStandalone: true

# Settings for Bitnami postgresql chart (if db.deployStandalone is true, ignored
#  otherwise)
postgresql:
  architecture: standalone
  auth:
    existingSecret: "litellm-postgresql-credentials-prod"
    secretKeys:
      adminPasswordKey: POSTGRES_POSTGRES_PASSWORD
      userPasswordKey: POSTGRES_PASSWORD
    username: litellm
    database: litellm  
    password: tPrGOgQdkzwpHAKeJDQj
    postgres-password: 5Mxn4BGmbflgtRCT9bKH 
  primary:
    persistence:
      enabled: true
      size: 50Gi
      storageClass: gp2
      selector: {}
    nodeSelector:
      if-llm-workload: "true"
    tolerations:
      - key: "if-monitoring-pod"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
      - key: "if-llm-workload"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"      
  extraEnvVarsCM: "litellm-prod-config"       
    # You should override these on the helm command line with
    #  `--set postgresql.auth.postgres-password=<some good password>,postgresql.auth.password=<some good password>`
    # password: 
    # postgres-password: 

    # A secret is created by this chart (litellm-helm) with the credentials that
    #  the new Postgres instance should use.
    # existingSecret: ""
    # secretKeys:
    #   userPasswordKey: password

# requires cache: true in config file
# either enable this or pass a secret for REDIS_HOST, REDIS_PORT, REDIS_PASSWORD or REDIS_URL
# with cache: true to use existing redis instance
redis:
  enabled: false
  architecture: standalone

# Prisma migration job settings
migrationJob:
  enabled: true # Enable or disable the schema migration Job
  retries: 3 # Number of retries for the Job in case of failure
  backoffLimit: 4 # Backoff limit for Job restarts
  disableSchemaUpdate: false # Skip schema migrations for specific environments. When True, the job will exit with code 0.
  annotations: {}
  ttlSecondsAfterFinished: 120
# Additional environment variables to be added to the deployment
envVars: {
    # USE_DDTRACE: "true"
}
